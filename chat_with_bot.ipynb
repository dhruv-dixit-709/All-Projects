{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something's missing?? (To be done later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After training, a different model architecture is used to make predictions on new input data, using the trained layers such as LSTM and TimeDistributed Dense which were defined globally during training. The code from chat_bot.py (particualrly the code pertaining to training the model) has to be pasted into this part. I skipped this part to make the entire code simpler to understand.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For initial filtering\n",
    "maxlen = 12\n",
    "maxLen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dictionary\n",
    "with open('dictionary.pkl', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the reverse dictionary\n",
    "with open('reverse_dictionary.pkl', 'rb') as f:\n",
    "    index_to_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question asked to the chatbot\n",
    "question = 'Hey! How are you doing?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing to make the data into the format required by the model, same as during training\n",
    "a = question.split()\n",
    "\n",
    "for pos,i in enumerate(a):\n",
    "    a[pos] = re.sub('[^a-zA-Z0-9 .,?!]', '', i)\n",
    "    a[pos]= re.sub(' +', ' ', i)\n",
    "    a[pos] = re.sub('([\\w]+)([,;.?!#&\\'\\\"-]+)([\\w]+)?', r'\\1 \\2 \\3', i)\n",
    "    \n",
    "    if len(i.split()) > maxlen:\n",
    "            a[pos] = (' ').join(a[pos].split()[:maxlen])\n",
    "            if '.' in a[pos]:\n",
    "                ind = a[pos].index('.')\n",
    "                a[pos] = a[pos][:ind+1]\n",
    "\n",
    "            if '?' in a[pos]:\n",
    "                ind = a[pos].index('?')\n",
    "                a[pos] = a[pos][:ind+1]\n",
    "\n",
    "            if '!' in a[pos]:\n",
    "                ind = a[pos].index('!')\n",
    "                a[pos] = a[pos][:ind+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ' '.join(a).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the question into an array of the corresponding indexes\n",
    "question = np.array([word_to_index[w] for w in question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "question = sequence.pad_sequences([question], maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model used to train, so that we define the variables (tensors) that ultimately go into the infernce model\n",
    "input_context = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_context')\n",
    "input_target = Input(shape = (maxLen, ), dtype = 'int32', name = 'output_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ctx_embed = embed_layer(input_context)\n",
    "input_tar_embed = embed_layer(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lstm, context_h, context_c = LSTM_encoder(input_ctx_embed)\n",
    "decoder_lstm, h, _ = LSTM_decoder(input_tar_embed, initial_state = [context_h, context_c],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dense(decoder_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for the input (question). Returns the final state vectors of the encoder LSTM\n",
    "context_model = Model(input_context, [context_h, context_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the inputs for the decoder LSTM\n",
    "target_h = Input(shape = (300, ))\n",
    "target_c = Input(shape = (300, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Takes in the embedding of the initial word passed as input into the decoder model (the 'BOS' tag), along with the final states of the encoder model, to output the corresponding sequences for 'BOS', and the new LSTM states.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, h, c = LSTM_decoder(input_tar_embed, initial_state = [target_h, target_c])\n",
    "output = dense(target)\n",
    "target_model = Model([input_target, target_h, target_c], [output, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the question to the encoder LSTM, to get the final encoder states of the encoder LSTM\n",
    "question_h, question_c = context_model.predict(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize the answer that will be generated for the 'BOS' input. Since we have used pre-padding for padding sequences, the last token in the 'answer' variable is initialised with the index for 'BOS'.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.zeros((1, maxLen))\n",
    "answer[0, -1] = word_to_index['BOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i keeps track of the length of the generated answer. This won't allow the model to genrate sequences with more than 20 words.\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new list to store the words generated at each time step\n",
    "answer_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to stop the model when 'EOS' tag is generated or when 20 time steps have passed.\n",
    "flag = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while flag != 1:\n",
    "    # Making the predictions for the given input token and encoder states\n",
    "    prediction, prediction_h, prediction_c = target_model.predict([answer, question_h, question_c])\n",
    "    \n",
    "    # From the generated predictions of shape (num_examples, maxLen, vocab_size), find the token with max probability\n",
    "    token_arg = np.argmax(prediction[0, -1, :])\n",
    "    \n",
    "    # Appending the corresponding word of the index to the answer_1 list\n",
    "    answer_1.append(index_to_word[token_arg])\n",
    "    \n",
    "    # Set flag to 1 if 'EOS' token is generated or 20 time steps have passed\n",
    "    if token_arg == word_to_index['EOS'] or i > 20:\n",
    "        flag = 1\n",
    "    \n",
    "    # Re-initialise the answer variable, and set the last token to the output of the current time step. This is then passed\n",
    "    # as input to the next time step, along with the LSTM states of the current time step\n",
    "    answer = np.zeros((1,maxLen))\n",
    "    answer[0, -1] = token_arg\n",
    "    question_h = prediction_h\n",
    "    question_c = prediction_c\n",
    "    \n",
    "    # Increment the count of the loop\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (' '.join(answer_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
