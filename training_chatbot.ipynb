{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = np.load('context_indexes.npy')\n",
    "final_target = np.load('target_indexes.npy')\n",
    "with open('dictionary.pkl', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideation\n",
    "*The indexes of the words start with 0. But when the sequences are padded later on, they too will be zeros. So, shift all the index values one position to the right, so that 0 is spared, and used only to pad the sequences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in word_to_index.items():\n",
    "    word_to_index[i] = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse dictionary\n",
    "index_to_word = {}\n",
    "for k,v in word_to_index.items():\n",
    "    index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_target_1 = final_target\n",
    "context_1 = context\n",
    "maxLen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the indexes of the context and target arrays too\n",
    "for i in final_target_1:\n",
    "    for pos,j in enumerate(i): i[pos] = j + 1\n",
    "for i in context_1:\n",
    "    for pos,j in enumerate(i): i[pos] = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading GloVe embeddings [50 Dimensions]\n",
    "def read_glove_vecs(file):\n",
    "    with open(file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            words.add(word)\n",
    "            word_to_vec_map[word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('.../data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideation\n",
    "*Since the indexes start from 1 and not 0, we add 1 to the no. of total words to get the vocabulary size (while initializing and populating arrays later on, this will be required)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix that will be used (50 dimensional GloVe Vector)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word,index in word_to_index_1.items():\n",
    "    try:\n",
    "        embedding_matrix[index, :] = word_to_vec_map[word.lower()]\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and populate the outputs to the Keras model.\n",
    "# The output is the same as the target, but shifted one time step to the left.\n",
    "outs = np.zeros((context_1.shape[0], maxLen, vocab_size))\n",
    "for pos,i in enumerate(final_target_1):\n",
    "    for pos1,j in enumerate(i):\n",
    "        if pos1 > 0:\n",
    "            outs[pos, pos1 - 1, j] = 1\n",
    "    if pos%1000 == 0: print ('{} entries completed'.format(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences so that they can be fed into the embedding layer\n",
    "final_target_1 = sequence.pad_sequences(final_target_1, maxlen = 20, dtype = 'int32', padding = 'post', truncating = 'post')\n",
    "context_1 = sequence.pad_sequences(context_1, maxlen = 20, dtype = 'int32', padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe vectors into the embedding layer\n",
    "embed_layer = Embedding(input_dim = vocab_size, output_dim = 50, trainable = True, )\n",
    "embed_layer.build((None,))\n",
    "embed_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randapa ahead..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Take deep breath*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder gloabal LSTM variables [300 Units]\n",
    "LSTM_cell = LSTM(300, return_state = True)\n",
    "LSTM_decoder = LSTM(300, return_state = True, return_sequences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dense layer that uses TimeDistributed wrapper to generate 'vocab_size' softmax outputs for each time step in the decoder lstm\n",
    "dense = TimeDistributed(Dense(vocab_size, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_context = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_context')\n",
    "input_target = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iski topi uske sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the inputs into the embedding layer\n",
    "input_ctx_embed = embed_layer(input_context)\n",
    "input_tar_embed = embed_layer(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the embeddings into the corresponding LSTM layers\n",
    "encoder_lstm, context_h, context_c = LSTM_cell(input_ctx_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder LSTM uses the final states from the Encoder LSTM as the initial state\n",
    "decoder_lstm, _, _ = LSTM_decoder(input_tar_embed, initial_state = [context_h, context_c],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dense(vocab_size, activation = 'softmax')(decoder_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_context, input_target], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([context_1, final_target_1], outs, epochs = 1000, batch_size = 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
